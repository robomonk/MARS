import pytest
import requests
import threading
import time
import uvicorn
import uuid
from unittest import mock
import os

# Agent specific imports (will be refined as we go)
# from agents.agent1.hypothesis_builder import HypothesisBuilder # Or the main class for Agent 1
# from agents.agent1.session_manager import SessionManager
from agents.agent1.agent import Agent1
from agents.agent2.main import app as agent2_app
from agents.agent3.main import app as agent3_app
from agents.agent2.collaboration import fetch_external_data as agent2_fetch_external_data
from google.cloud import bigquery as gcp_bigquery, storage as gcp_storage, aiplatform_v1beta1 as gcp_aiplatform
from agents.agent2.models import Hypothesis as Agent2Hypothesis, Protocol as Agent2Protocol
from agents.agent3.models import AbstractProtocol as Agent3AbstractProtocol, BuildPlan as Agent3BuildPlan

# 1. Initial researcher prompt
INITIAL_PROMPT = "I think nutrient levels in soil affect crop yield"

# 2. Expected structure for Agent 1's hypothesis output (as a dictionary)
# This is based on what agents.agent1.agent.Agent1.handle_message might save.
# We will assert that the saved data conforms to these keys and value types.
# The actual values will depend on the interaction flow.
EXPECTED_AGENT1_HYPOTHESIS_SCHEMA = {
    "hypothesis_id": str, # This will be generated by our mock
    "session_id": str,    # This will come from Agent 1
    "title": str,
    "summary": str,
    "details": (dict, str), # Can be a dict or a string based on Agent1's code
    "conversation_summary": list,
    # "status": str, # Optional: if Agent 1 adds a status
}

# Placeholder for Agent 2's specific mock search results
# This will be populated per test or in a more specific fixture.
# For now, just define its structure.
MOCK_SEARCH_RESULTS_TEMPLATE = {
    "Public datasets for Test premise: Nutrient levels in soil directly impact crop yield": "mock_dataset_available_result",
    "Python libraries for Test premise: Nutrient levels in soil directly impact crop yield": "mock_library_available_result",
    "Availability of computational model for Test premise: Nutrient levels in soil directly impact crop yield": "mock_model_exists_result",
    "Public datasets for Test premise: Specific nutrients (e.g., Nitrogen, Phosphorus) have different impacts": "mock_specific_nutrients_dataset_result",
    "Python libraries for Test premise: Specific nutrients (e.g., Nitrogen, Phosphorus) have different impacts": "mock_specific_nutrients_library_result",
    "Availability of computational model for Test premise: Specific nutrients (e.g., Nitrogen, Phosphorus) have different impacts": "mock_specific_nutrients_model_result"
}

# It's good practice to have a clear base URL for services
AGENT2_BASE_URL = "http://127.0.0.1:8002" # Different port for Agent 2
AGENT3_BASE_URL = "http://127.0.0.1:8003" # Different port for Agent 3

# Helper function to run FastAPI app in a separate thread
def run_fastapi_app(app, host="127.0.0.1", port=8000):
    config = uvicorn.Config(app, host=host, port=port, log_level="info")
    server = uvicorn.Server(config)
    server.run()

@pytest.fixture(scope="class")
def agent2_server(self):
    thread = threading.Thread(target=run_fastapi_app, args=(agent2_app, "127.0.0.1", 8002), daemon=True)
    thread.start()
    # Wait for the server to start
    time.sleep(2) # Adjust as needed
    yield
    # Teardown (optional, as daemon threads will exit with main thread)

@pytest.fixture(scope="class")
def agent3_server(self):
    thread = threading.Thread(target=run_fastapi_app, args=(agent3_app, "127.0.0.1", 8003), daemon=True)
    thread.start()
    # Wait for the server to start
    time.sleep(2) # Adjust as needed
    yield
    # Teardown

@pytest.mark.usefixtures("agent2_server", "agent3_server")
class TestAgentIntegration:
    @pytest.fixture(scope="class", autouse=True)
    def setup_class_mocks(self):
        # --- Existing Firestore Mocks (keep these) ---
        self.mock_firestore_db = mock.MagicMock()
        self.agent1_saved_hypotheses = {}
        def mock_save_final_hypothesis(session_id, hypothesis_data):
            print(f"MOCK_FIRESTORE: Saving hypothesis for session {session_id}")
            hyp_id = str(uuid.uuid4())
            self.agent1_saved_hypotheses[hyp_id] = {
                **hypothesis_data,
                "hypothesis_id": hyp_id,
                "session_id": session_id
            }
            return hyp_id
        patcher_sm_save = mock.patch('agents.agent1.session_manager.SessionManager.save_final_hypothesis', side_effect=mock_save_final_hypothesis)
        patcher_sm_db = mock.patch('agents.agent1.session_manager.SessionManager.db', new_callable=mock.PropertyMock(return_value=self.mock_firestore_db))
        patcher_fc_db = mock.patch('agents.agent1.firestore_client.db', new=self.mock_firestore_db)
        self.mock_sm_save = patcher_sm_save.start()
        self.mock_sm_db_prop = patcher_sm_db.start()
        self.mock_fc_db_direct = patcher_fc_db.start()
        # --- End of Existing Firestore Mocks ---

        # --- Mock for Agent 2's fetch_external_data ---
        self.mock_agent2_search_results = {} # To store expected results for tests

        def mock_fetch_external_data_side_effect(search_queries: list[str]) -> dict[str, str]:
            results = {}
            for query in search_queries:
                results[query] = self.mock_agent2_search_results.get(query, f"mocked_default_result_for_{query.replace(' ', '_')}")
            return results

        patcher_agent2_fetch = mock.patch('agents.agent2.collaboration.fetch_external_data', side_effect=mock_fetch_external_data_side_effect)
        self.mock_agent2_fetch = patcher_agent2_fetch.start()
        # --- End of Mock for Agent 2 ---

        # --- Mocks for Agent 3's GCP Clients ---
        self.mock_gcp_bigquery_client = mock.MagicMock(spec=gcp_bigquery.Client)
        self.mock_gcp_storage_client = mock.MagicMock(spec=gcp_storage.Client)
        self.mock_gcp_vertex_notebook_client = mock.MagicMock(spec=gcp_aiplatform.NotebookServiceClient)

        # Mock the creation of dataset and bucket for chained calls if necessary
        mock_dataset = mock.MagicMock(spec=gcp_bigquery.Dataset)
        self.mock_gcp_bigquery_client.return_value.create_dataset.return_value = mock_dataset
        mock_bucket = mock.MagicMock(spec=gcp_storage.Bucket)
        self.mock_gcp_storage_client.return_value.create_bucket.return_value = mock_bucket
        # Mock Vertex AI operation result for create_instance
        mock_vertex_operation = mock.MagicMock()
        mock_vertex_operation.result.return_value = None # Simulate successful completion
        self.mock_gcp_vertex_notebook_client.return_value.create_instance.return_value = mock_vertex_operation


        patcher_bq = mock.patch('agents.agent3.execution_engine.bigquery.Client', return_value=self.mock_gcp_bigquery_client)
        patcher_gcs = mock.patch('agents.agent3.execution_engine.storage.Client', return_value=self.mock_gcp_storage_client)
        patcher_vertex_nb = mock.patch('agents.agent3.execution_engine.aiplatform.NotebookServiceClient', return_value=self.mock_gcp_vertex_notebook_client)

        self.patcher_bq_client = patcher_bq.start()
        self.patcher_gcs_client = patcher_gcs.start()
        self.patcher_vertex_nb_client = patcher_vertex_nb.start()
        # --- End of Mocks for Agent 3 ---

        yield # Pytest fixture yield

        # Stop all patchers
        patcher_sm_save.stop()
        patcher_sm_db.stop()
        patcher_fc_db.stop()
        patcher_agent2_fetch.stop()
        self.patcher_bq_client.stop()
        self.patcher_gcs_client.stop()
        self.patcher_vertex_nb_client.stop()

    def test_placeholder(self):
        # A simple test to ensure the setup runs
        assert True

    def test_agent1_hypothesis_generation_and_save(self):
        # --- Step 1: Instantiate Agent 1 and simulate interaction ---
        test_session_id = str(uuid.uuid4())
        agent1 = Agent1(session_id=test_session_id)

        # Simulate a conversation
        response1 = agent1.handle_message(INITIAL_PROMPT)
        assert response1 is not None # Basic check on response

        # Message to trigger finalization
        finalize_response = agent1.handle_message("finalize please")
        assert finalize_response is not None
        assert "Hypothesis saved with ID" in finalize_response # Check if mock save message is in response

        # --- Step 2: Assert hypothesis saved to (mocked) Firestore ---
        self.mock_sm_save.assert_called() # Asserts it was called at least once

        assert len(self.agent1_saved_hypotheses) > 0, "No hypothesis was saved by Agent 1's mock"

        # Retrieve the *first* hypothesis saved in this test run.
        # Note: If multiple tests save hypotheses, this dict will grow.
        # Consider clearing it per test or fetching by a known ID if tests become more complex.
        # For now, assuming this test is the first to save or we take the latest.
        # Let's find the one for our current session_id.
        saved_hypothesis_id = None
        for hyp_id, data in self.agent1_saved_hypotheses.items():
            if data.get("session_id") == test_session_id:
                saved_hypothesis_id = hyp_id
                break

        assert saved_hypothesis_id is not None, f"No hypothesis found for session_id {test_session_id}"

        saved_hypothesis = self.agent1_saved_hypotheses[saved_hypothesis_id]

        # Validate structure against EXPECTED_AGENT1_HYPOTHESIS_SCHEMA
        for key, expected_type in EXPECTED_AGENT1_HYPOTHESIS_SCHEMA.items():
            assert key in saved_hypothesis, f"Key '{key}' not found in saved hypothesis"
            # For tuple expected_type (e.g. (dict, str)), check if isinstance of any type in tuple
            if isinstance(expected_type, tuple):
                assert isinstance(saved_hypothesis[key], expected_type), \
                    f"Value for key '{key}' has type {type(saved_hypothesis[key])}, expected one of {expected_type}"
            else:
                assert isinstance(saved_hypothesis[key], expected_type), \
                    f"Value for key '{key}' has type {type(saved_hypothesis[key])}, expected {expected_type}"

        # Specific check for 'hypothesis_id' as it's crucial
        assert "hypothesis_id" in saved_hypothesis
        assert isinstance(saved_hypothesis["hypothesis_id"], str)

        # Store the validated hypothesis for the next step (Agent 2)
        # We'll need to pass this to the next test or retrieve it.
        # For now, let's assume we can access it via self.agent1_saved_hypotheses using its ID.
        # Or, more simply for sequential steps in one test method later:
        # self.last_validated_hypothesis_for_agent2 = saved_hypothesis
        # (This would require merging test methods or careful state management if tests are separate)

        # For now, this test focuses on Agent 1. We'll handle passing data in the next step.
        print(f"Agent 1 Test: Successfully validated saved hypothesis: {saved_hypothesis_id}")

    def test_agent2_protocol_generation(self):
        # --- Prerequisite: Get a hypothesis from Agent 1 ---
        # This part is similar to test_agent1_hypothesis_generation_and_save
        # In a full suite, this might be a dependent test or use a fixture.
        test_session_id_for_agent1 = str(uuid.uuid4())
        agent1 = Agent1(session_id=test_session_id_for_agent1)
        agent1.handle_message(INITIAL_PROMPT)
        agent1.handle_message("finalize please")

        self.mock_sm_save.assert_called()
        assert len(self.agent1_saved_hypotheses) > 0, "Agent 1 did not save any hypothesis"

        agent1_output_id = None
        for hyp_id, data in self.agent1_saved_hypotheses.items():
            if data.get("session_id") == test_session_id_for_agent1:
                agent1_output_id = hyp_id
                break
        assert agent1_output_id is not None, "Could not find saved hypothesis from Agent 1"
        agent1_hypothesis_data = self.agent1_saved_hypotheses[agent1_output_id]

        # --- Step 3: Prepare hypothesis for Agent 2 and verify receipt (by sending it) ---
        # Transform Agent 1's output to Agent 2's Hypothesis input model
        # Agent 1's mock save_final_hypothesis returns an ID, but Agent 2 expects hypothesis_id in the body.
        # The current Agent1 mock for save_final_hypothesis already adds 'hypothesis_id'

        # We need to ensure the structure from agent1_hypothesis_data maps to Agent2Hypothesis.
        # Agent1's "details" could be a dict or string. Agent2's Hypothesis model has:
        # hypothesis_id: str, statement: str, core_assumptions: List[str], description: str
        # Let's assume agent1_hypothesis_data['summary'] is the 'statement' for Agent 2
        # and agent1_hypothesis_data['title'] can be 'description'.
        # 'core_assumptions' might need to be derived or mocked if not directly in Agent1's output.

        # For this test, let's construct a valid Agent2Hypothesis payload.
        # The actual mapping might be more complex in reality.
        payload_for_agent2 = {
            "hypothesis_id": agent1_hypothesis_data["hypothesis_id"], # from agent1 mock
            "statement": agent1_hypothesis_data.get("summary", INITIAL_PROMPT),
            "core_assumptions": agent1_hypothesis_data.get("core_assumptions", ["Nutrient levels in soil directly impact crop yield", "Specific nutrients (e.g., Nitrogen, Phosphorus) have different impacts"]), # Example
            "description": agent1_hypothesis_data.get("title", "Test Hypothesis from Agent 1"),
            # status, evaluation_metrics, data_sources, metadata can use defaults or be added if needed
        }

        # Validate this payload structure with Agent2Hypothesis before sending
        try:
            agent2_input_hypothesis = Agent2Hypothesis(**payload_for_agent2)
        except Exception as e:
            pytest.fail(f"Failed to create Agent2Hypothesis from Agent 1 output: {e}")

        # --- Step 4: Mock external APIs (Google Search) ---
        # Configure the mock search results for Agent 2's `fetch_external_data`
        # Based on the core_assumptions in payload_for_agent2
        self.mock_agent2_search_results.clear() # Clear previous results
        for assumption in agent2_input_hypothesis.core_assumptions:
            self.mock_agent2_search_results[f"Public datasets for Test premise: {assumption}"] = f"mock_dataset_for_{assumption[:20]}"
            self.mock_agent2_search_results[f"Python libraries for Test premise: {assumption}"] = f"mock_library_for_{assumption[:20]}"
            self.mock_agent2_search_results[f"Availability of computational model for Test premise: {assumption}"] = f"mock_model_for_{assumption[:20]}"

        # --- Step 5: Send to Agent 2 and Assert Protocol ---
        agent2_response = requests.post(f"{AGENT2_BASE_URL}/design_experiment/", json=agent2_input_hypothesis.model_dump())

        assert agent2_response.status_code == 200, \
            f"Agent 2 POST request failed: {agent2_response.status_code} - {agent2_response.text}"

        # Validate the response against Agent2Protocol Pydantic model
        try:
            protocol_data = agent2_response.json()
            generated_protocol = Agent2Protocol(**protocol_data)
        except Exception as e:
            pytest.fail(f"Failed to parse Agent 2 response as Agent2Protocol: {e}. Response: {agent2_response.text}")

        assert generated_protocol.linked_hypothesis_id == agent2_input_hypothesis.hypothesis_id
        assert len(generated_protocol.validation_steps) > 0 # Should have steps based on premises

        # Assert feasibility assessment based on mocked search results
        # The mock search results were set up to indicate availability
        assert generated_protocol.feasibility_assessment is not None
        assert generated_protocol.feasibility_assessment.data_obtainability == 'PUBLIC' # Based on mock_fetch_external_data logic
        assert generated_protocol.feasibility_assessment.tools_availability == 'OPEN_SOURCE' # Based on mock_fetch_external_data logic
        assert generated_protocol.feasibility_assessment.confidence_score > 0.5 # Example assertion

        # Store for Agent 3 (similar to Agent 1, will refine data passing later)
        # self.last_validated_protocol_for_agent3 = generated_protocol
        print(f"Agent 2 Test: Successfully generated and validated protocol: {generated_protocol.protocol_id}")

    def test_agent3_build_plan_generation_and_approval(self):
        # --- Prerequisite: Get an Experiment Protocol from Agent 2 ---
        # This combines simplified steps from previous tests.
        # Agent 1 part
        test_session_id_agent1 = str(uuid.uuid4())
        agent1 = Agent1(session_id=test_session_id_agent1)
        agent1.handle_message(INITIAL_PROMPT)
        agent1.handle_message("finalize please")
        self.mock_sm_save.assert_called()
        # Simplified retrieval - this might be fragile if other tests modify agent1_saved_hypotheses
        # A better way would be to fetch based on test_session_id_agent1 as in previous test.
        # For now, using next(iter()) if only one hypothesis is expected from this flow.
        agent1_output_id = None
        for hyp_id, data in self.agent1_saved_hypotheses.items():
            if data.get("session_id") == test_session_id_agent1: # Find by current session
                agent1_output_id = hyp_id
                break
        assert agent1_output_id is not None, "Could not find saved hypothesis from Agent 1 for this test"
        agent1_hypothesis_data = self.agent1_saved_hypotheses[agent1_output_id]


        agent2_payload = {
            "hypothesis_id": agent1_hypothesis_data["hypothesis_id"],
            "statement": agent1_hypothesis_data.get("summary", INITIAL_PROMPT),
            "core_assumptions": ["Nutrient levels affect yield", "Nitrogen is key"], # Example assumptions for Agent 2
            "description": agent1_hypothesis_data.get("title", "Test Hypothesis"),
        }
        agent2_input_hypothesis = Agent2Hypothesis(**agent2_payload)

        # Configure Agent 2 mocks
        self.mock_agent2_search_results.clear()
        for assumption in agent2_input_hypothesis.core_assumptions:
            self.mock_agent2_search_results[f"Public datasets for Test premise: {assumption}"] = "mock_ds"
            self.mock_agent2_search_results[f"Python libraries for Test premise: {assumption}"] = "mock_lib"
            self.mock_agent2_search_results[f"Availability of computational model for Test premise: {assumption}"] = "mock_model"

        agent2_response = requests.post(f"{AGENT2_BASE_URL}/design_experiment/", json=agent2_input_hypothesis.model_dump())
        assert agent2_response.status_code == 200, f"Agent 2 call failed: {agent2_response.text}"
        agent2_protocol = Agent2Protocol(**agent2_response.json())
        # --- End of Prerequisite ---

        # --- Step 6: Send Protocol to Agent 3 and Get Build Plan ---
        # Transform Agent 2's Protocol to Agent 3's AbstractProtocol input
        agent3_abstract_protocol_payload = {
            "protocol_id": agent2_protocol.protocol_id,
            "title": f"Build plan for {agent2_protocol.protocol_id}",
            "research_question": agent2_input_hypothesis.statement,
            "data_requirement": "structured_sql_db", # Critical for plan_translator logic in Agent3
            "computation_steps": [step.description for step in agent2_protocol.validation_steps]
        }

        try:
            agent3_input_protocol = Agent3AbstractProtocol(**agent3_abstract_protocol_payload)
        except Exception as e:
            pytest.fail(f"Failed to create Agent3AbstractProtocol: {e}")

        agent3_plan_response = requests.post(
            f"{AGENT3_BASE_URL}/receive_experiment_protocol",
            json=agent3_input_protocol.model_dump()
        )

        assert agent3_plan_response.status_code == 200, \
            f"Agent 3 /receive_experiment_protocol failed: {agent3_plan_response.status_code} - {agent3_plan_response.text}"

        try:
            build_plan_data = agent3_plan_response.json()
            generated_build_plan = Agent3BuildPlan(**build_plan_data)
        except Exception as e:
            pytest.fail(f"Failed to parse Agent 3 BuildPlan response: {e}. Response: {agent3_plan_response.text}")

        assert generated_build_plan.protocol_id == agent2_protocol.protocol_id
        assert generated_build_plan.status == 'pending_approval'
        assert len(generated_build_plan.steps) > 0

        # --- Step 7: Programmatically Approve the Build Plan ---
        plan_id = generated_build_plan.plan_id

        agent3_confirm_response = requests.post(f"{AGENT3_BASE_URL}/build_plan/{plan_id}/confirm")

        assert agent3_confirm_response.status_code == 200, \
             f"Agent 3 /build_plan/{plan_id}/confirm failed: {agent3_confirm_response.status_code} - {agent3_confirm_response.text}"

        confirmation_data = agent3_confirm_response.json()
        assert confirmation_data["plan_id"] == plan_id
        assert confirmation_data["confirmed"] is True
        assert "approved" in confirmation_data["message"].lower()

        # Store for Agent 3 execution step (will be used in the next test method)
        # self.last_approved_build_plan = generated_build_plan
        print(f"Agent 3 Test: Successfully generated build plan {plan_id} and approved it.")

    def test_agent3_build_plan_execution(self):
        # --- Prerequisite: Get an Approved Build Plan from Agent 3 ---
        # This combines all previous steps. Refactoring this into a helper
        # or fixture would be good for larger test suites.

        # Agent 1 part
        test_session_id_agent1 = str(uuid.uuid4())
        agent1 = Agent1(session_id=test_session_id_agent1)
        agent1.handle_message(INITIAL_PROMPT)
        agent1.handle_message("finalize please")
        self.mock_sm_save.assert_called()
        # Find the correct hypothesis for this session
        agent1_output_id = None
        for hyp_id, data in self.agent1_saved_hypotheses.items():
            if data.get("session_id") == test_session_id_agent1:
                agent1_output_id = hyp_id
                break
        assert agent1_output_id is not None, "Agent 1 hypothesis not found for session"
        agent1_hypothesis_data = self.agent1_saved_hypotheses[agent1_output_id]

        # Agent 2 part
        agent2_payload = {
            "hypothesis_id": agent1_hypothesis_data["hypothesis_id"],
            "statement": agent1_hypothesis_data.get("summary", INITIAL_PROMPT),
            "core_assumptions": ["Soil nutrients affect crop yield", "Nitrogen is a key nutrient"], # Example assumptions
            "description": agent1_hypothesis_data.get("title", "Test Hypothesis"),
        }
        agent2_input_hypothesis = Agent2Hypothesis(**agent2_payload)
        self.mock_agent2_search_results.clear()
        for assumption in agent2_input_hypothesis.core_assumptions: # Use the actual assumptions
            self.mock_agent2_search_results[f"Public datasets for Test premise: {assumption}"] = f"mock_ds_for_{assumption[:10]}"
            self.mock_agent2_search_results[f"Python libraries for Test premise: {assumption}"] = f"mock_lib_for_{assumption[:10]}"
            self.mock_agent2_search_results[f"Availability of computational model for Test premise: {assumption}"] = f"mock_model_for_{assumption[:10]}"

        agent2_response = requests.post(f"{AGENT2_BASE_URL}/design_experiment/", json=agent2_input_hypothesis.model_dump())
        assert agent2_response.status_code == 200
        agent2_protocol = Agent2Protocol(**agent2_response.json())

        # Agent 3 Plan Generation part
        agent3_abstract_protocol_payload = {
            "protocol_id": agent2_protocol.protocol_id,
            "title": f"Execution test for {agent2_protocol.protocol_id}",
            "research_question": agent2_input_hypothesis.statement,
            "data_requirement": "structured_sql_db", # This should trigger BigQuery client call in Agent 3
            "computation_steps": [step.description for step in agent2_protocol.validation_steps] # Corrected access
        }
        agent3_input_protocol = Agent3AbstractProtocol(**agent3_abstract_protocol_payload)
        agent3_plan_response = requests.post(f"{AGENT3_BASE_URL}/receive_experiment_protocol", json=agent3_input_protocol.model_dump())
        assert agent3_plan_response.status_code == 200
        generated_build_plan = Agent3BuildPlan(**agent3_plan_response.json())

        # Agent 3 Plan Approval part
        plan_id = generated_build_plan.plan_id
        agent3_confirm_response = requests.post(f"{AGENT3_BASE_URL}/build_plan/{plan_id}/confirm")
        assert agent3_confirm_response.status_code == 200
        # --- End of Prerequisite ---

        # --- Step 8: Execute the Build Plan and Assert GCP Calls ---
        # Mock environment variables for Agent 3 execution context
        mock_gcp_project_id = "test-gcp-project"
        mock_gcp_location = "test-location"

        # Reset mock calls for GCP clients before execution to have clean assertions for this test
        self.mock_gcp_bigquery_client.reset_mock()
        self.mock_gcp_storage_client.reset_mock()
        self.mock_gcp_vertex_notebook_client.reset_mock()

        # Patch os.environ for the duration of this execution call
        with mock.patch.dict(os.environ, {"GCP_PROJECT_ID": mock_gcp_project_id, "GCP_LOCATION": mock_gcp_location}):
            agent3_execute_response = requests.post(f"{AGENT3_BASE_URL}/build_plan/{plan_id}/execute")

        assert agent3_execute_response.status_code == 200, \
            f"Agent 3 /build_plan/{plan_id}/execute failed: {agent3_execute_response.status_code} - {agent3_execute_response.text}"

        execute_response_data = agent3_execute_response.json()
        assert execute_response_data["message"] == "Build plan executed successfully"
        assert execute_response_data["plan_id"] == plan_id
        assert execute_response_data["new_status"] == "completed"

        expected_bq_step_name = f'experiment_data_{agent2_protocol.protocol_id[:8]}'
        found_bq_step = False
        for step in generated_build_plan.steps:
            if step.type == "bigquery_dataset" and step.action == "create_resource":
                found_bq_step = True
                # Check if BigQuery client was instantiated (patched to return self.mock_gcp_bigquery_client)
                self.patcher_bq_client.assert_called()

                # Check if create_dataset was called on the mock instance
                self.mock_gcp_bigquery_client.create_dataset.assert_called_once()
                call_args = self.mock_gcp_bigquery_client.create_dataset.call_args

                dataset_arg = call_args[0][0]
                assert isinstance(dataset_arg, gcp_bigquery.Dataset)
                # Dataset name is project_id.dataset_id in the client library
                assert dataset_arg.dataset_id == f"{expected_bq_step_name}" # Corrected, project_id is implicit
                assert dataset_arg.project == mock_gcp_project_id # Client uses project_id at initialization usually

                # The client itself forms project.dataset_id, so we check the dataset_id part and then project attribute
                # Or, if the client expects dataset_id to be fully qualified, then it's:
                # assert dataset_arg.full_dataset_id == f"{mock_gcp_project_id}.{expected_bq_step_name}"
                # Let's assume the mock is set up on client.create_dataset(dataset_ref)
                # and dataset_ref.dataset_id is 'expected_bq_step_name'
                # and dataset_ref.project is 'mock_gcp_project_id'

                # The actual BQ client dataset object would have dataset_id = 'name' and project = 'project_id'
                # The string representation is project.name
                # Let's check what the code in agent3 execution_engine actually does:
                # dataset_ref = bigquery.DatasetReference(project_id, dataset_name)
                # client.create_dataset(dataset_ref, timeout=30)
                # So dataset_arg will be a DatasetReference or Dataset object.
                # If it's a Dataset object, its dataset_id is just the name.
                # If it's a DatasetReference, its dataset_id is also just the name.
                assert dataset_arg.dataset_id == expected_bq_step_name
                # The mock object `self.mock_gcp_bigquery_client` is the return_value of the patched `bigquery.Client()` call.
                # So, the call `self.mock_gcp_bigquery_client.create_dataset` is what we check.
                # The argument passed to it is `dataset_object`.
                # `dataset_object = bigquery.Dataset(dataset_ref)`
                # `dataset_object.description = details.get("description")`
                # So, dataset_arg.dataset_id should be `expected_bq_step_name`
                # and dataset_arg.project should be `mock_gcp_project_id`.

                if step.details and "description" in step.details:
                     assert dataset_arg.description == step.details["description"]
                break

        assert found_bq_step, "BigQuery dataset creation step was not found in the plan or not executed as expected."

        print(f"Agent 3 Test: Successfully executed build plan {plan_id} and verified GCP (mock) calls.")
